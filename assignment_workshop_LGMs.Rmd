---
title: "Workshop on Latent Growth Modeling in Lavaan"
author: "Lifespan Cognitive Dynamics Lab"
date: "November 2024"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) # for the answer sheet

# knitr::opts_chunk$set(echo = FALSE, eval = FALSE) # for the assignment
```

Welcome to this tutorial on Latent Growth Models (LGMs).

Latent Growth Models are structural equation models (SEM) that incorporate repeated observed measures of one variable to estimate the unobserved growth trajectory of that variable. 
Latent variables are unobserved variables. In latent growth models the growth trajectories are represented by the intercept and slope parameters, which are considered latent variables.

Lavaan notation cheat sheet: https://osf.io/q2prk

The dataset used for this workshop is called wisc. The WISC-V Test (Wechsler Intelligence Scale for Children) is an IQ test administered to children between ages 6 and 16. It provides five primary index scores, namely Verbal Comprehension Index, Visual Spatial Index, Fluid Reasoning Index, Working Memory Index, and Processing Speed Index. In the workshop you will work on a subset containing: Verbal Comprehension Index, Processing Speed Index and the total. 

Now it's time to get started!

## Content {.tabset .tabset-pills}

### Getting started

The first step on this LGM adventure is to make sure all necessary programs and packages are installed. If you do not have R and/or its user-friendly interface, RStudio, installed, please do so via [this website](https://posit.co/download/rstudio-desktop/).

Then, within R, you need to install and load the lavaan package, which you are going to fit LGMs with.

<font size="4">**Assignment 1: Install and load the lavaan package.**</font>

```{r class.source = "fold-show", getpackages, message=FALSE,warning=FALSE, echo=TRUE}
if(!require("pacman")){install.packages("pacman",repos = "http://cran.us.r-project.org")}

pacman::p_load(lavaan, tidyverse, here, reshape2)
```

Now download the WISC data from the shared folder. Make sure that your R environment is linked to the folder in which you saved the data by setting your working directory.


<font size="4">**Assignment 2: Load the WISC data into R and explore which data are in the data file.**</font>

```{r class.source = "fold-show", loaddata, message=FALSE,warning=FALSE,include=TRUE, echo = TRUE}
path <- "/Users/njudd/projects/LGC_Workshop/" #change to the LGC_Workshop folder

setwd(path)
wisc <- read.csv(paste0(path,"wisc.csv"))[,-1]

head(wisc)         #first 6 rows
colnames(wisc)     #names of columns
dim(wisc)          #number of rows and columns
sapply(wisc,class) #number of each column

```

For now, we are only going to analyze data from the verbal subtest (indicated by a column name that starts with "Verbal").

<font size="4">**Assignment 3: Subset the data such that the dataset you work with only contains the id numbers and the scores on the verbal subtest across the four measurements. Your subset should thus contain five columns.**</font>

```{r class.source = "fold-show", subsetdata, message=FALSE,warning=FALSE}
wisc_verbal <- wisc[,c("ID","Verbal_T6","Verbal_T7","Verbal_T9","Verbal_T11")]
```

As you may have noticed when exploring the data, they are in wide format, that is, they contain one row per participant and the different measurements are in separate columns. To fit models in Lavaan, this wide format is necessary. Yet, for plotting, it is easier if data are in long format, with one row per participant per measurement.

<font size="4">**Assignment 4: Reshape the data subset from wide to long format.**</font>

```{r class.source = "fold-show", reshapedata, message=FALSE,warning=FALSE}
wisc_verbal_long <- wisc_verbal %>% 
  pivot_longer(!ID, names_to = "wave", values_to = "verbal") #test
```
Now that we have prepared the data we want to model, let's plot them!

<font size="4">**Assignment 5: Plot the data with the four measurements on the x-axis, the score on the verbal subtest on the y-axis, and a line for each subject.**</font>

```{r class.source = "fold-show", plotdata, message=FALSE,warning=FALSE,include=T}
wisc_verbal_long$wave = factor(wisc_verbal_long$wave, levels=c("Verbal_T6","Verbal_T7","Verbal_T9","Verbal_T11"))

ggplot(wisc_verbal_long, aes(wave, verbal, group=ID, fill=ID, color=ID)) +
  geom_point() + 
  geom_line() +
  theme_classic(base_size = 15) + # adding a classic theme; https://ggplot2.tidyverse.org/reference/ggtheme.html
  theme(legend.position = "none") + # getting rid of legend
  labs(x = "Wave", y = "Score on Verbal Subtest")
```

To enable you to check whether your model implementation is correct in a later step, it is good to first formulate expectations based on the plotted data.

<font size="4">**Assignment 6: Describe what you see. What is the average score at each of the four measurements?  Do subjects in- or decrease across measurements? Are there individual differences in these effects?**</font>

You're ready to move on to do some model fitting now. <a href="#top">Click here to go back to the top</a> to move on to the next module.





### Basic LGM in Lavaan

You are now going to actually fit an LGM in lavaan. We start simple.

<font size="4">**Assignment 7: Start by creating an empty string that you call linear_growth_model. Then try to implement an LGM in which you estimate (1) intercepts for each of the four time points, and (2) a *linear* slope. See the slides and the cheat sheet at the top for examples and hints.**</font>

```{r class.source = "fold-show", createmodel, message=FALSE,warning=FALSE}
# Create LGM
linear_growth_model <- '
  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11'

```

<font size="4">**Assignment 8: Fit the model you created in assignment 7 to the verbal subset data using the growth() function and plot results using summary().**</font>

The lavaan::growth function is a wrapper function (for lavaan::lavaan) that simplifies the specification of growth models. See details in the help file: `?lavaan::growth`, for more info see `?lavaan::lavOptions`.


```{r class.source = "fold-show", outputres, message=FALSE,warning=FALSE,include=TRUE}
# Fit LGM
fit_linear_growth_model <- growth(linear_growth_model, data=wisc_verbal,missing='fiml')
# Output results
summary(fit_linear_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)
```

<font size="4">**Assignment 9: How is model fit?**</font>

<font size="4">**Assignment 10: What is the average verbal score at baseline? How does this compare to the expectations you formulated in assignment 6?**</font>

<font size="4">**Assignment 11: What is the average change per wave?**</font>

<font size="4">**Assignment 12: Are there individual differences in the score at baseline? And in how much individuals change?**</font>

<font size="4">**Assignment 13: What does the score at baseline tell you about how much individuals change?**</font>

Great! You have interpreted your first LGM output. Let's make it a little more difficult in the <a href="#top">next module</a>.




### Different Shapes of Growth

In the previous module, we modeled a linear growth model. Yet, it is also possible to model non-linear growth in lavaan like a quadratic trajectory. For this you need to add a third parameter called a quadratic term that will get the same loadings as for the slope but squared. 
To do this, you need to specify one more latent variable in your model called quadratic term. The quadratic term is given loadings that are the squares of the loadings for the slope.

<font size="4">**Assignment 14: Create a quadratic growth model, fit it to the verbal data, and output results.**</font>

```{r class.source = "fold-show", quadmodel, message=FALSE,warning=FALSE,include=TRUE}

# Create quadratic growth model
quad_growth_model <- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
                      s =~ 0*Verbal_T6 + 1*Verbal_T7 + 2*Verbal_T9 + 3*Verbal_T11
                      q =~ 0*Verbal_T6 + 1*Verbal_T7 + 4*Verbal_T9 + 9*Verbal_T11'
# Fit model
fit_quad_growth_model <- growth(quad_growth_model, data=wisc_verbal,missing='fiml')
# Output results
summary(fit_quad_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)
#

```

<font size="4">**Assignment 15: How is model fit?**</font>

<font size="4">**Assignment 16: What is the average verbal score at baseline? Does this estimate differ from the estimate in assignment 10? And from your expectations?**</font>

<font size="4">**Assignment 17: What is the shape of change across measurements?**</font>

<font size="4">**Assignment 18: You have an error message. What does it mean? Can you see where is the issue in the output?**</font>


It is also possible to model non-linear growth in lavaan with no hypothesis on the shape. To do so, you fix the loadings of the first and last measurement, but freely estimate the middle ones.

<font size="4">**Assignment 19: Create a non-linear (basis) growth model, fit it to the verbal data, and output results. Then compare the model fit with the previous models.**</font>

```{r class.source = "fold-show", basisoutput, message=FALSE,warning=FALSE,include=TRUE}
# Create non-linear growth model
basis_growth_model <- 'i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
                       s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11'
# Fit model
fit_basis_growth_model <- growth(basis_growth_model, data=wisc_verbal,missing='fiml')
# Output results
summary(fit_basis_growth_model, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)

```

<font size="4">**Assignment 20: Perform a statistical test to compare model fit. Does the linear or non-linear model fit better?**</font>

```{r class.source = "fold-show", comparefit2, message=FALSE,warning=FALSE,include=TRUE}
# Compare model fit
anova(fit_basis_growth_model,fit_linear_growth_model)
#Here it looks like the basis model fits better
```

Congrats! You can now choose the best shape for your trajectory. Let's add predictors in the <a href="#top">next module</a>.


### Predictors and outcomes

One may be interested in what predicts baseline scores and/or change. To assess this, one can add predictors in the growth model. One hypothesis could be that the level of education of the mother predicts the development of verbal comprehension. 

<font size="4">**Assignment 21: Add educational level of the mother (mo_edu in the data file) as predictor of baseline scores and change in the non-linear model.**</font>

```{r class.source = "fold-show", addcov, message=FALSE,warning=FALSE,include=TRUE}
# Specify model
basis_growth_model_cov <- ' 
  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 
  s~mo_edu
  i~mo_edu
  '
```

<font size="4">**Assignment 22: Fit the model from assignment 19 to the verbal data and output results. Does mother's education predict baseline scores? And what about change across measurements?**</font>

```{r class.source = "fold-show", fitcov, message=FALSE,warning=FALSE,include=TRUE}
# Fit model
fit_basis_growth_model_cov <- growth(basis_growth_model_cov, data=wisc,missing='fiml')
# Output results
summary(fit_basis_growth_model_cov, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)
```

<font size="4">**Assignment 23: Add processing speed at 11 as an outcome of changes in verbal comprehension. In other words, test if the slopes of verbal change predict the level of processing speed at 11.**</font>

```{r class.source = "fold-show", addout, message=FALSE,warning=FALSE,include=TRUE}
# Specify model
basis_growth_model_covO <- ' 
  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 
  Pspeed_T11~s
  Pspeed_T11~1
'
# Fit model
fit_basis_growth_model_covO <- growth(basis_growth_model_covO, data=wisc,missing='fiml')
# Output results
summary(fit_basis_growth_model_covO, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)
```


<font size="4">**Assignment 24: Perform the same steps as in assignments 20 and 21 but now for processing speed at baseline. Does processing speed relate to verbal baseline scores? And to change?**</font>

```{r class.source = "fold-show", fitcov2, message=FALSE,warning=FALSE,include=TRUE}
# Specify model
basis_growth_model_covP <- ' 
  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 
  s~Pspeed_T6
  i~Pspeed_T6
  i~~s
'
# Fit model
fit_basis_growth_model_covP <- growth(basis_growth_model_covP, data=wisc,missing='fiml')
# Output results
summary(fit_basis_growth_model_covP, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)
```


**Extra**

Time-invariant predictors are predictors of the individual differences in intercepts and slopes.They are often measurement at baseline (e.g., family income) or person-specific characteristics where value is constant over time (e.g., biological sex, country of origin).
For instance, in the previous assignments, level of education of the mother and processing speed at 6 are time-invariant predictors. 
Time-varying predictors are predictors of the outcome at each time point. In our example for instance we would need measurements at T6, T7, T9 and T11 

<font size="4">**Assignment 25: Use processing speed as a time-varying predictor of the verbal measurement at each time point. How are the intercept and slope of the verbal measures? Does processing speed predict verbal measures the same way across time points?**</font>

```{r class.source = "fold-show", timevar, message=FALSE,warning=FALSE,include=TRUE}
# Specify model
basis_growth_model_tvp <- ' 
  i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 
  Verbal_T6~Pspeed_T6
  Verbal_T7~Pspeed_T7
  Verbal_T9~Pspeed_T9
  Verbal_T11~Pspeed_T11
  '
# Fit LGM
fit_basis_growth_model_tvp <- growth(basis_growth_model_tvp, data=wisc,missing='fiml')
# Output results
summary(fit_basis_growth_model_tvp, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)

```

You're ready to add a bit more complexity to those models. <a href="#top">Click here to go back to the top</a> to move on to the next module.


### Uneven time intervals

Sometimes data collection does not happen every year, the intervals between two measures might be uneven. In the wisc data it seems that they measured the children at 6, 7, 9 and 11 years old. 

<font size="4">**Assignment 26: Perform the same steps as in assignments 7, 8 and 14 but take into account the uneven time intervals in your loadings. Compare the three fits of the three models. Do you have the same result as in the previous assignments?**</font>

```{r class.source = "fold-show", unevenint, message=FALSE,warning=FALSE,include=TRUE}
# Specify linear model
linear_growth_model_uneven <- ' i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
                                s =~ 6*Verbal_T6 + 7*Verbal_T7 + 9*Verbal_T9 + 11*Verbal_T11'

# Fit LGM
fit_linear_growth_model_uneven <- growth(linear_growth_model_uneven, data=wisc_verbal,missing='fiml')
# Output results
summary(fit_linear_growth_model_uneven, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)

# Specify quadratic model
quad_growth_model_uneven <- ' i =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
                                s =~ 6*Verbal_T6 + 7*Verbal_T7 + 9*Verbal_T9 + 11*Verbal_T11
                                q =~ 36*Verbal_T6 + 49*Verbal_T7 + 81*Verbal_T9 + 121*Verbal_T11'

# Fit LGM
fit_quad_growth_model_uneven <- growth(quad_growth_model_uneven, data=wisc_verbal,missing='fiml')
# Output results
summary(fit_quad_growth_model_uneven, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)

#You should get the warning message "In lav_object_post_check(object) : lavaan WARNING: some estimated lv variances are negative" 

# Compare linear model fit between the model not taking into account the uneven intervals and the one taking into account the uneven intervals
anova(fit_linear_growth_model_uneven,fit_linear_growth_model)
#You cannot compare these models

# Compare model fit between linear trajectory with uneven intervals and the basis model
anova(fit_linear_growth_model_uneven,fit_basis_growth_model)
#Here it looks like the basis model is preferred. 


```

A recommendation is that, if time intervals are unequal, it may be better to use measured time.





### Intercept and slopes relations

Now that you know how to estimate the trajectory of one variable you are able to estimate the trajectory of two variables and see how they interact. 

<font size="4">**Assignment 27: Create two non-linear (basis) growth models, one for verbal and one for processing speed. Correlate the changes of the two metrics. Are their slopes correlated?**</font>

```{r class.source = "fold-show", twotraj, message=FALSE,warning=FALSE,include=TRUE}
# Specify model
basis_growth_model_cor_ver_pro <- ' 
  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 
  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11
  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 
  s_verbal ~~ s_processpeed'

# Fit LGM
fit_basis_growth_model_cor_ver_pro <- growth(basis_growth_model_cor_ver_pro, data=wisc,missing='fiml')
# Output results
summary(fit_basis_growth_model_cor_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)

```

<font size="4">**Assignment 28: Within that model, explore how one metrics baseline level predicts the changes in the other. How do they predict each other?**</font>

```{r class.source = "fold-show", twotraj2, message=FALSE,warning=FALSE,include=TRUE}
# Specify model
basis_growth_model_pred_ver_pro <- ' 
  i_verbal =~ 1*Verbal_T6 + 1*Verbal_T7 + 1*Verbal_T9 + 1*Verbal_T11
  s_verbal =~ 0*Verbal_T6 + Verbal_T7 + Verbal_T9 + 1*Verbal_T11 
  i_processpeed =~ 1*Pspeed_T6 + 1*Pspeed_T7 + 1*Pspeed_T9 + 1*Pspeed_T11
  s_processpeed =~ 0*Pspeed_T6 + Pspeed_T7 + Pspeed_T9 + 1*Pspeed_T11 
  s_verbal ~ i_processpeed
  s_processpeed ~ i_verbal'

# Fit LGM
fit_basis_growth_model_pred_ver_pro <- growth(basis_growth_model_pred_ver_pro, data=wisc,missing='fiml')
# Output results
summary(fit_basis_growth_model_pred_ver_pro, fit.measures = TRUE, rsquare = TRUE, standardized = TRUE)

```

Let's finish with simulation of data. <a href="#top">Click here to go back to the top</a> to move on to the next module.



### LEAP data [extra]

Here we will fit a LGM to synthetic LEAP data! 

<font size="4">**Assignment 29: Load LEAP data & try to make a longitudinal raincloud plot for ssp with ggrain**</font>

HINT: You might need the melt function from the package reshape2! [Here](https://www.njudd.com/raincloud-ggrain/) is a link to the ggrain documentation.

```{r class.source = "fold-show", leapPlt, message=FALSE,warning=FALSE,include=TRUE}


pacman::p_load(reshape2, ggrain)

leap_synthdata <- read.csv(paste0(path,"leap_synthdata.csv"))

# I thought this was a subject ID... it clearly is not
leap_synthdata[leap_synthdata$subjects == 949903300427,]


# now we make the data frame long with melt on ssp
leap_synthdata_long <- melt(leap_synthdata, id.vars = "subjects", measure.vars = colnames(leap_synthdata)[str_detect(colnames(leap_synthdata), "ssp")])

# plotting the data
# ggplot(leap_synthdata_long, aes(variable, value, group = subjects, color = subjects)) +
#   geom_point() +
#   geom_line() +
#   theme_classic(base_size = 15) +
#   theme(legend.position = "none") +
#   labs(x = "Wave", y = "Score on ssp")

# now lets make raincloud plots
# 
# leap_synthdata_long$subjects <- as.character(leap_synthdata_long$subjects)
# leap_2x <- leap_synthdata_long[leap_synthdata_long$variable != "t3_ssp_total",]

ggplot(leap_synthdata_long[leap_synthdata_long$variable != "t3_ssp_total",], aes(x = variable, y = value, fill = variable)) +
  geom_rain(id.long.var = "subjects", rain.side = "f1x1") +
  theme_classic(base_size = 15) +
  labs(x = "Wave", y = "Score on ssp")


```

<font size="4">**Assignment 30: Make a LGC model with ssp**</font>

```{r class.source = "fold-show", leap_lgc, message=FALSE,warning=FALSE,include=TRUE}

fit_lgm_ssp<- '
i_ssp =~ 1*t1_ssp_total + 1*t2_ssp_total + 1*t3_ssp_total
s_ssp =~ 0*t1_ssp_total + 1*t2_ssp_total + 2*t3_ssp_total
t1_ssp_total~~ervar1*t1_ssp_total
t2_ssp_total~~ervar2*t2_ssp_total
t3_ssp_total~~ervar3*t3_ssp_total
'
fit_fit_lgm_ssp <- growth(fit_lgm_ssp, data=leap_synthdata,missing='fiml',estimator='mlr')
# Output results
summary(fit_fit_lgm_ssp, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE,ci=T)

```

<font size="4">**Assignment 31: Choose a exogenous variable and see how it impacts the change of ssp!**</font>


### Simulate your data [extra]

Data simulation is a useful tool to test your models and do power analysis, 

<font size="4">**Assignment 32: Simulate a linear growth model with 4 waves. The intercepts should have a mean of 100 and a variance of 15. The slope should have a mean of 5 and a variance of 10. The intercept-slope covariance should have a mean of 3.**</font>

```{r class.source = "fold-show", simdat, message=FALSE,warning=FALSE,include=TRUE}
# Specify linear model
sim_lgm_4waves<- '
i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4 

i~100*1         ##<## this specifies the intercept, or the mean at T1
i~~15*i         ##<## this specifies the intercept variance at T1

s~5*1           ##<## this specifies the slope, or change per wave
s~~10*s         ##<## this specifies the slope variance 

i~~-3*s         ##<## this specifies the intercept/slope covariance 

'
sim_lgm_4waves_dat <- simulateData(sim_lgm_4waves, sample.nobs = 500)

#Sanity check of the column means. As expected the scores increase ~ 5 points per wave
colMeans(sim_lgm_4waves_dat)

#Plotting. Now you can start tweaking parameters to see what the data looks like
sim_lgm_4waves_dat$id<-as.factor(1:nrow(sim_lgm_4waves_dat))
theme_set(theme_grey(base_size = 18)) #increase text size
plotdat<-melt(sim_lgm_4waves_dat)

ggplot(plotdat, aes(variable,value,group=id, fill=id, color=id)) +
  geom_point() + 
  geom_line() +
  theme_classic(base_size = 15) + # adding a classic theme; https://ggplot2.tidyverse.org/reference/ggtheme.html
  theme(legend.position = "none") + # getting rid of legend
  labs(x = "Wave", y = "Score")

```

```{r class.source = "fold-show", fitsimdat, message=FALSE,warning=FALSE,include=TRUE}
# Specify linear model
fit_lgm_4waves<- '
i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4 
i~~s
'

# Fit LGM
fit_fit_lgm_4waves <- growth(fit_lgm_4waves, data=sim_lgm_4waves_dat,missing='fiml')
# Output results
summary(fit_fit_lgm_4waves, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE,ci=T)

parTable(fit_fit_lgm_4waves)   #Parameters are recovered nicely

```

You might want to simulate different means for the intercepts and slopes of different groups (e.g., female and male). You can simulate those groups at the same time by using a list: c().

<font size="4">**Assignment 33: Simulate a linear growth model with three groups.**</font>

```{r class.source = "fold-show", simdatgroups, message=FALSE,warning=FALSE,include=TRUE}
# Specify linear model
sim_lgm_4waves_MG<- '
i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4 

i~c(100,90,80)*1         ##<## this specifies the intercept, or the mean at T1
i~~c(15,15,2)*i         ##<## this specifies the intercept variance at T1

s~c(4,0,10)*1           ##<## this specifies the slope, or change per wave
s~~c(10,10,5)*s         ##<## this specifies the slope variance 

i~~c(3,3,3)*s         ##<## this specifies the intercept/slope covariance 
'

sim_sim_lgm_4waves_MG <- simulateData(sim_lgm_4waves_MG, sample.nobs = c(100,100,20))

#Sanity check of the column means. 
colMeans(sim_sim_lgm_4waves_MG)


# Specify linear model
fit_lgm_4waves_MG<- '
i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4 
'

# Fit LGM
fit_sim_sim_lgm_4waves_MG <- growth(fit_lgm_4waves_MG, data=sim_sim_lgm_4waves_MG,group='group')

# Output results
summary(fit_sim_sim_lgm_4waves_MG, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE,ci=T)

#Plotting. Now you can start tweaking parameters to see what the data looks like
sim_sim_lgm_4waves_MG$id<-as.factor(1:nrow(sim_sim_lgm_4waves_MG))
sim_sim_lgm_4waves_MG$group<-as.factor(sim_sim_lgm_4waves_MG$group)
theme_set(theme_grey(base_size = 18)) #increase text size
plotdat<-melt(sim_sim_lgm_4waves_MG,id=c("id","group"))

ggplot(plotdat, aes(variable,value,group=id)) +
  geom_point() + 
  geom_line() +
  theme_classic(base_size = 15) + # adding a classic theme; https://ggplot2.tidyverse.org/reference/ggtheme.html
  theme(legend.position = "none") + # getting rid of legend
  labs(x = "Wave", y = "Score") +
  facet_wrap(~group)

```


Simulating data can also be used for power analysis to test what effect size would be detected with your sample size and your model. 

<font size="4">**Assignment 34: Use the previous simulate data model to test the effect size of a predictor on the slope.**</font>

```{r class.source = "fold-show", poweranalysis, message=FALSE,warning=FALSE,include=TRUE}

#General specifications
nsims<-100                               #Define the number of iterations
samplesize<- 400                         #Define the sample size 
modelcomp<-data.frame(matrix(NA,nsims,7)) #create empty object to store logicals for whether the correct model is preferred
colnames(modelcomp)<-c('effect size','#LRT delta chi square','LRT delta df','LRT pv-value','delta AIC','AIC logical')


# Create a matrix for the effect size tested
effectsize <- c(0,0.05,0.1,0.15,0.2,0.25,0.3)

power_effectsize<-data.frame(matrix(NA,length(effectsize),4)) 
colnames(power_effectsize)<-c('effect_size','powerLRT','powerAIC','powerAICthres')

# Display the resulting data frame
print(effectsize)
a=0
for (j in 1:length(effectsize)) {
  effectsize[j]
  
for (i in 1:nsims) #initiate loop
{
  
  model_test<-NA   #change model output back to NA
  model_test_constraint<-NA   #change model output back to NA
  
  #Specify the true model
  #Simulate data for a bivariate Latent Change Score model
  model_sim_lgm_4waves<-
   paste('i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
          s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4 

          i~100*1         ##<## this specifies the intercept, or the mean at T1
          i~~15*i         ##<## this specifies the intercept variance at T1

          s~5*1           ##<## this specifies the slope, or change per wave
          s~~10*s         ##<## this specifies the slope variance 

          i~~-3*s         ##<## this specifies the intercept/slope covariance 

          pred~10*1         ##<## this specifies the pred, or the mean at T1
          pred~~5*pred         ##<## this specifies the pred variance at T1

          s~ ',effectsize[j],'*pred
         
         ')

#Simulate data 
simdat_lgm_4waves<-simulateData(model_sim_lgm_4waves,sample.nobs = samplesize,meanstructure = T,empirical=TRUE)       

#check if the parameters make sense with the number you provided 
colMeans(simdat_lgm_4waves)
  
  model_test <- '
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
    i~~s
    s~ a*pred
'
  fit_model_test <- growth(model_test, data=simdat_lgm_4waves, estimator='mlr',missing='fiml')
  
  model_test_constraint <- '
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
    i~~s
    s~ 0*pred
'
  fit_model_test_constraint <- growth(model_test_constraint, data=simdat_lgm_4waves, estimator='mlr',missing='fiml')
    
  #Likelihood ratio test
  LRTcomp<-anova(fit_model_test,fit_model_test_constraint)
  modelaAIC<-AIC(fit_model_test) #grab AIC for the true model fit
  modelbAIC<-AIC(fit_model_test_constraint) #grab AIC for the competitor model fit
  
  modelcomp[i+a,1]<-effectsize[j]
  modelcomp[i+a,2]<-LRTcomp[2,5] #LRT chi square
  modelcomp[i+a,3]<-LRTcomp[2,6] #LRT delta df
  modelcomp[i+a,4]<-LRTcomp[2,7] #LRT p val
  modelcomp[i+a,5]<-modelbAIC-modelaAIC #delta AIC: positive means A preferred over B
  modelcomp[i+a,6]<-modelaAIC<modelbAIC #logical, does the alternative model have a
}
  power_effectsize[j,1]<- effectsize[j]
  powerLRT<-(sum(modelcomp[c((a+1):(a+100)),4]<0.05)/nsims)*100 #sum logical: divide by nsims to get percentage power
  power_effectsize[j,2]<- powerLRT
  powerAIC<-(sum(as.numeric(modelcomp[c((a+1):(a+100)),6]))/nsims)*100
  power_effectsize[j,3]<- powerAIC
  AICthres<-10
  powerAICthres<-sum(modelcomp[c((a+1):(a+100)),5]>10)/nsims*100
  power_effectsize[j,4]<- powerAICthres
  a=a+100
}

ggplot(power_effectsize,aes(effect_size,powerLRT))+
  geom_point()

```





